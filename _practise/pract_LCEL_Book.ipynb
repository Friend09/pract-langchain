{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL\n",
    "\n",
    "https://python.langchain.com/docs/expression_language/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api key\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: OutputParsers and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. SwiftTech\\n2. MobileGenius\\n3. PocketPal\\n4. TechMobility\\n5. Cloud9Mobile\\n6. DigiHive\\n7. MobileMaven\\n8. AppTastic\\n9. OnTheGoTech\\n10. TouchTech\\n11. MobileEdge\\n12. PocketRocket\\n13. TechSavvy\\n14. MobileMatrix\\n15. CloudMinds\\n16. AppXcelerate\\n17. PocketTech\\n18. OnTheMoveTech\\n19. MobileSphere\\n20. TechTrailblazers']\n"
     ]
    }
   ],
   "source": [
    "# 7.1.1\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Suggest some names for my {subject} startup. \\n\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\n",
    "            \"format_instructions\": format_instructions,\n",
    "            \"subject\": \"Mobile\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Monument\": string  // The monument mentioned in the answer.\n",
      "\t\"city\": string  // the city in which this monument is present.\n",
      "\t\"architect\": string  // the architect of the monument.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 7.1.2 Custom OutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"Monument\",\n",
    "        description=\"The monument mentioned in the answer.\",\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"city\", description=\"the city in which this monument is present.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"architect\",\n",
    "        description=\"the architect of the monument.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting property name enclosed in double quotes: line 2 column 27 (char 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/utils/json.py:176\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/utils/json.py:147\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/utils/json.py:160\u001b[0m, in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/utils/json.py:120\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 2 column 27 (char 28)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m     10\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m output_parser\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat_instructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoose a random Indian state and pen down some cities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:178\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    171\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1625\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1622\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1623\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1624\u001b[0m         Output,\n\u001b[0;32m-> 1625\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1635\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:179\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    171\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    181\u001b[0m         config,\n\u001b[1;32m    182\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:221\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain/output_parsers/structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     96\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/utils/json.py:178\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    176\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting property name enclosed in double quotes: line 2 column 27 (char 28)"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "    answer the users question as best as possible. \\n\n",
    "    {format_instructions}\\n {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\n",
    "            \"format_instructions\": format_instructions,\n",
    "            \"question\": \"Choose a random Indian state and pen down some cities\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(state='Andhra Pradesh', cities=['Visakhapatnam', 'Vijayawada', 'Guntur', 'Nellore', 'Kurnool', 'Rajahmundry', 'Kakinada', 'Tirupati', 'Anantapur', 'Kadapa'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.13\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import List\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    state: str = Field(description=\"name of an Indian State\")\n",
    "    cities: List[str] = Field(description=\"Indian cities in that state\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=State)\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "template = \"\"\"answer the users question as best as possible and generate output. \\n\n",
    "{format_instructions} \\n{questions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm = OpenAI(temperature=0, api_key=OPENAI_API_KEY)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"questions\": \"State is Andhra Pradesh\",\n",
    "        \"format_instructions\": format_instructions,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coversational buffer memory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: hi there, my name is Raghu\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'human_input': 'hi there, my name is Raghu',\n",
       " 'chat_history': '',\n",
       " 'text': \" Hello Raghu, it's nice to meet you. I am a chatbot designed to have conversations with humans. How can I assist you today?\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.invoke(\"hi there, my name is Raghu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "Human: hi there, my name is Raghu\n",
      "AI:  Hello Raghu, it's nice to meet you. I am a chatbot designed to have conversations with humans. How can I assist you today?\n",
      "Human: write a song about me\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'human_input': 'write a song about me',\n",
       " 'chat_history': \"Human: hi there, my name is Raghu\\nAI:  Hello Raghu, it's nice to meet you. I am a chatbot designed to have conversations with humans. How can I assist you today?\",\n",
       " 'text': \" Sure, Raghu, here's a little song I came up with just for you:\\n\\nVerse 1:\\nRaghu, oh Raghu, with a heart of gold\\nYour smile shines bright, never growing old\\nYou light up the room with your infectious laugh\\nAnd your kindness and warmth, they're hard to surpass\\n\\nChorus:\\nOh Raghu, you're one of a kind\\nA true gem, so hard to find\\nYour presence brings joy, wherever you go\\nYou're a shining star, don't you know?\\n\\nVerse 2:\\nWith your passion and drive, you reach for the stars\\nAnd your determination, it knows no bars\\nYou inspire those around you, to be their best\\nAnd with you by their side, they feel truly blessed\\n\\nChorus:\\nOh Raghu, you're one of a kind\\nA true gem, so hard to find\\nYour presence brings joy, wherever you go\\nYou're a shining star, don't you know?\\n\\nBridge:\\nIn a world full of chaos, you're a calming force\\nAnd your friendship, it's a true gift, of course\\nYou bring light to the darkness, and hope to the despair\\nRaghu, you're a true friend, beyond compare\\n\\nCh\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"write a song about me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "Human: hi there, my name is Raghu\n",
      "AI:  Hello Raghu, it's nice to meet you. I am a chatbot designed to have conversations with humans. How can I assist you today?\n",
      "Human: write a song about me\n",
      "AI:  Sure, Raghu, here's a little song I came up with just for you:\n",
      "\n",
      "Verse 1:\n",
      "Raghu, oh Raghu, with a heart of gold\n",
      "Your smile shines bright, never growing old\n",
      "You light up the room with your infectious laugh\n",
      "And your kindness and warmth, they're hard to surpass\n",
      "\n",
      "Chorus:\n",
      "Oh Raghu, you're one of a kind\n",
      "A true gem, so hard to find\n",
      "Your presence brings joy, wherever you go\n",
      "You're a shining star, don't you know?\n",
      "\n",
      "Verse 2:\n",
      "With your passion and drive, you reach for the stars\n",
      "And your determination, it knows no bars\n",
      "You inspire those around you, to be their best\n",
      "And with you by their side, they feel truly blessed\n",
      "\n",
      "Chorus:\n",
      "Oh Raghu, you're one of a kind\n",
      "A true gem, so hard to find\n",
      "Your presence brings joy, wherever you go\n",
      "You're a shining star, don't you know?\n",
      "\n",
      "Bridge:\n",
      "In a world full of chaos, you're a calming force\n",
      "And your friendship, it's a true gift, of course\n",
      "You bring light to the darkness, and hope to the despair\n",
      "Raghu, you're a true friend, beyond compare\n",
      "\n",
      "Ch\n",
      "Human: what is the song about?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'human_input': 'what is the song about?',\n",
       " 'chat_history': \"Human: hi there, my name is Raghu\\nAI:  Hello Raghu, it's nice to meet you. I am a chatbot designed to have conversations with humans. How can I assist you today?\\nHuman: write a song about me\\nAI:  Sure, Raghu, here's a little song I came up with just for you:\\n\\nVerse 1:\\nRaghu, oh Raghu, with a heart of gold\\nYour smile shines bright, never growing old\\nYou light up the room with your infectious laugh\\nAnd your kindness and warmth, they're hard to surpass\\n\\nChorus:\\nOh Raghu, you're one of a kind\\nA true gem, so hard to find\\nYour presence brings joy, wherever you go\\nYou're a shining star, don't you know?\\n\\nVerse 2:\\nWith your passion and drive, you reach for the stars\\nAnd your determination, it knows no bars\\nYou inspire those around you, to be their best\\nAnd with you by their side, they feel truly blessed\\n\\nChorus:\\nOh Raghu, you're one of a kind\\nA true gem, so hard to find\\nYour presence brings joy, wherever you go\\nYou're a shining star, don't you know?\\n\\nBridge:\\nIn a world full of chaos, you're a calming force\\nAnd your friendship, it's a true gift, of course\\nYou bring light to the darkness, and hope to the despair\\nRaghu, you're a true friend, beyond compare\\n\\nCh\",\n",
       " 'text': ' The song is about you, Raghu. It celebrates your unique qualities and the positive impact you have on those around you.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what is the song about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: hi there, my name is Raghu\\nAI:  Hello Raghu, it's nice to meet you. I am a chatbot designed to have conversations with humans. How can I assist you today?\\nHuman: write a song about me\\nAI:  Sure, Raghu, here's a little song I came up with just for you:\\n\\nVerse 1:\\nRaghu, oh Raghu, with a heart of gold\\nYour smile shines bright, never growing old\\nYou light up the room with your infectious laugh\\nAnd your kindness and warmth, they're hard to surpass\\n\\nChorus:\\nOh Raghu, you're one of a kind\\nA true gem, so hard to find\\nYour presence brings joy, wherever you go\\nYou're a shining star, don't you know?\\n\\nVerse 2:\\nWith your passion and drive, you reach for the stars\\nAnd your determination, it knows no bars\\nYou inspire those around you, to be their best\\nAnd with you by their side, they feel truly blessed\\n\\nChorus:\\nOh Raghu, you're one of a kind\\nA true gem, so hard to find\\nYour presence brings joy, wherever you go\\nYou're a shining star, don't you know?\\n\\nBridge:\\nIn a world full of chaos, you're a calming force\\nAnd your friendship, it's a true gift, of course\\nYou bring light to the darkness, and hope to the despair\\nRaghu, you're a true friend, beyond compare\\n\\nCh\\nHuman: what is the song about?\\nAI:  The song is about you, Raghu. It celebrates your unique qualities and the positive impact you have on those around you.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(api_key=OPENAI_API_KEY, temperature=0))\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"hi, I am Raghu\"}, {\"output\": \"hello Raghu? how can I help\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "chat = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human introduces themselves as Raghu and the AI responds by asking how it can help.\n",
      "Human: what is GPT4o model?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is GPT4o model?',\n",
       " 'history': '\\nThe human introduces themselves as Raghu and the AI responds by asking how it can help.',\n",
       " 'response': ' GPT4o is a natural language processing model developed by OpenAI. It is the successor to GPT-3 and is designed to generate human-like text responses. It has 175 billion parameters, making it one of the largest language models in existence. It is trained on a diverse range of internet text data and can perform a variety of tasks such as language translation, text completion, and question-answering. Is there anything specific you would like to know about GPT4o?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(input=\"what is GPT4o model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human introduces themselves as Raghu and the AI responds by asking how it can help. The human then asks about GPT4o, a natural language processing model developed by OpenAI with 175 billion parameters. The AI explains that it is trained on internet text data and can perform various tasks. The AI asks if there is anything specific the human would like to know about GPT4o.\n",
      "Human: what is the difference b/w GPT4 and GPT4o\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the difference b/w GPT4 and GPT4o',\n",
       " 'history': '\\nThe human introduces themselves as Raghu and the AI responds by asking how it can help. The human then asks about GPT4o, a natural language processing model developed by OpenAI with 175 billion parameters. The AI explains that it is trained on internet text data and can perform various tasks. The AI asks if there is anything specific the human would like to know about GPT4o.',\n",
       " 'response': ' GPT4o is an improved version of GPT4, with a larger number of parameters and improved training methods. GPT4o has 175 billion parameters, while GPT4 has 175 billion parameters. This allows GPT4o to have a better understanding of natural language and perform more complex tasks. Additionally, GPT4o is trained on a larger and more diverse dataset, including internet text data, which helps it to better understand and generate human-like language.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(input=\"what is the difference b/w GPT4 and GPT4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi, I am Raghu'),\n",
       " AIMessage(content='hello Raghu? how can I help'),\n",
       " HumanMessage(content='what is GPT4o model?'),\n",
       " AIMessage(content=' GPT4o is a natural language processing model developed by OpenAI. It is the successor to GPT-3 and is designed to generate human-like text responses. It has 175 billion parameters, making it one of the largest language models in existence. It is trained on a diverse range of internet text data and can perform a variety of tasks such as language translation, text completion, and question-answering. Is there anything specific you would like to know about GPT4o?'),\n",
       " HumanMessage(content='what is the difference b/w GPT4 and GPT4o'),\n",
       " AIMessage(content=' GPT4o is an improved version of GPT4, with a larger number of parameters and improved training methods. GPT4o has 175 billion parameters, while GPT4 has 175 billion parameters. This allows GPT4o to have a better understanding of natural language and perform more complex tasks. Additionally, GPT4o is trained on a larger and more diverse dataset, including internet text data, which helps it to better understand and generate human-like language.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of Canada is Ottawa.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(country=\"Canada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of Canada?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of Canada is Ottawa.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(country=\"Canada\", callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 file handler\n",
    "from langchain.callbacks import FileCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = \"../files/logs/output.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'United States',\n",
       " 'text': '\\n\\nThe capital of United States is Washington, D.C.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}? \")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "\n",
    "answer = chain.invoke({\"country\": \"United States\"})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-28 15:26:08.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1m{'country': 'United States', 'text': '\\n\\nThe capital of United States is Washington, D.C.'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Custom callbacks\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class MyHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self,\n",
    "        serialized: \"Dict[str, Any]\",\n",
    "        prompts: \"List[str]\",\n",
    "        *,\n",
    "        run_id: \"UUID\",\n",
    "        parent_run_id: \"Optional[UUID]\" = None,\n",
    "        tags: \"Optional[List[str]]\" = None,\n",
    "        metadata: \"Optional[Dict[str, Any]]\" = None,\n",
    "        **kwargs: \"Any\"\n",
    "    ):\n",
    "        print(\"We are starting of with this prompt -->\", prompts)\n",
    "\n",
    "    def on_llm_end(\n",
    "        self,\n",
    "        response: \"LLMResult\",\n",
    "        *,\n",
    "        run_id: \"UUID\",\n",
    "        parent_run_id: \"Optional[UUID]\" = None,\n",
    "        **kwargs: \"Any\"\n",
    "    ):\n",
    "        print(\"The LLM is done\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-langchain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are starting of with this prompt --> ['What is the capital of Argentina?']\n",
      "The LLM is done generations=[[Generation(text='\\n\\nThe capital of Argentina is Buenos Aires.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 16, 'completion_tokens': 9, 'prompt_tokens': 7}, 'model_name': 'gpt-3.5-turbo-instruct'} run=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of Argentina is Buenos Aires.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler = MyHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(country=\"Argentina\", callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: RAG Framework and Vector Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
