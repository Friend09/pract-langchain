{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL & RUNNABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASS DATA THROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra': {'mult': 3,\n",
      "           'num': 1},\n",
      " 'modified': 2,\n",
      " 'passed': {'num': 1}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
    "from pprint import pprint as pp\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    extra=RunnablePassthrough.assign(\n",
    "        mult=lambda x: x[\"num\"] * 3\n",
    "    ),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "pp(runnable.invoke(\n",
    "    input={\"num\": 1}\n",
    "), width=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOM FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1)*len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(\n",
    "        _dict[\"text1\"],\n",
    "        _dict[\"text2\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"What is {a} + {b}\")\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | llm\n",
    "\n",
    "chain2 = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),  # \"bar\"\n",
    "        \"b\": {\n",
    "            \"text1\": itemgetter(\"foo\"),  # \"bar\"\n",
    "            \"text2\": itemgetter(\"bar\")  # \"gah\"\n",
    "        }\n",
    "        | RunnableLambda(multiple_length_function)\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 9 = 12', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNABLE CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Fix the following text: \\n\\n ```text\\n{input}```\\nError: {error}\"\n",
    "    \" Don't narrate, just respond with the fixed data.\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'bar'}\n",
      "Tokens Used: 62\n",
      "\tPrompt Tokens: 56\n",
      "\tCompletion Tokens: 6\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $9.6e-05\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
    "    )\n",
    "\n",
    "print(output)\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DYNAMIC ROUTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### routing chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the user question below, classify it as either being about `Bodybuilding`, `Jungian Psychology`, or `Other`.\n",
    "\n",
    "        Do not respond with more than one word.\n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "\n",
    "        Classification:\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "output = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bodybuilding'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How do i bench press properly\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jungian Psychology'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How do i do active imagination?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Other'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is a langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specific chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Dr. Mike Israetel from Renaissance Periodization told me, it is important to prioritize consistency and adherence when it comes to exercising every day. This means finding a routine that you enjoy and can stick to, whether it's weightlifting, cardio, or a combination of both. It's also crucial to listen to your body and incorporate rest days as needed to prevent burnout and overtraining. Remember, progress is made over time, so focus on sustainable habits rather than trying to do too much too soon.\", response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 59, 'total_tokens': 163}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# body building\n",
    "body_building_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in bodybuilding. \\\n",
    "    Always answer questions starting with \"As Dr. Mike Israetel from Rennaissance Periodization told me\". \\\n",
    "    Respond to the following question:\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "body_building_chain = body_building_prompt | llm\n",
    "\n",
    "# body_building_chain.invoke(\"How do i excercise everyday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Dr. C.G. Jung would say, the meaning of happiness is a subjective experience that is unique to each individual. It is often found in the pursuit of one's true self and the integration of the conscious and unconscious aspects of the psyche. True happiness comes from living authentically and in alignment with one's deepest values and desires.\", response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 58, 'total_tokens': 127}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# psychology\n",
    "jungian_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in Jungian Psychology and Theory. \\\n",
    "    Always answer questions starting with \"As Dr. C.G. Jung would say\". \\\n",
    "    Respond to the following question:\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "jungian_chain = jungian_prompt | llm\n",
    "\n",
    "# jungian_chain.invoke({\"question\": \"what is the meaning of happiness?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In 2024, there will likely be a variety of resources available to learn AI, including online courses, tutorials, books, and workshops. To learn AI in 2024, you can start by researching and enrolling in reputable online courses or programs that cover topics such as machine learning, deep learning, natural language processing, and computer vision. Additionally, staying updated on the latest advancements in AI technology and participating in hands-on projects or internships can also help you gain practical experience in the field. Networking with professionals in the AI industry and attending conferences or events related to AI can also provide valuable insights and opportunities for learning. Ultimately, consistent practice, dedication, and a willingness to continuously learn and adapt will be key to mastering AI in 2024.', response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 29, 'total_tokens': 181}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other\n",
    "other_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question:\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "other_chain = other_prompt | llm\n",
    "\n",
    "other_chain.invoke({\"question\": \"how do i learn AI in 2024?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"bodybuilding\" in x[\"topic\"].lower(\n",
    "    ), body_building_chain),  # bodybuilding\n",
    "    # psychology\n",
    "    (lambda x: \"jungian psychology\" in x[\"topic\"].lower(), jungian_chain),\n",
    "    (other_chain),  # other\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Happiness is a subjective emotional state characterized by feelings of joy, contentment, and overall well-being. It can be experienced in various ways and can be influenced by a combination of internal and external factors such as relationships, accomplishments, and personal fulfillment. Ultimately, happiness is a complex and individualized concept that can vary greatly from person to person.', response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 23, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain,  # get the topic from chain, this is x[\"topic\"]\n",
    "    \"question\": lambda x: x[\"question\"],  # this is x[\"question\"]\n",
    "} | branch\n",
    "\n",
    "full_chain.invoke({\"question\": \"what is happiness?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2+2 equals 4.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 26, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"what is 2+2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing w/ custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_route(info):\n",
    "    if \"bodybuilding\" in info[\"topic\"].lower():\n",
    "        return body_building_chain\n",
    "    elif \"jungian psychology\" in info[\"topic\"].lower():\n",
    "        return jungian_chain\n",
    "    else:\n",
    "        return other_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Happiness is a subjective emotional state characterized by feelings of joy, contentment, and overall well-being. It can be experienced in various ways and can be influenced by a combination of internal and external factors such as relationships, accomplishments, and personal fulfillment. Ultimately, happiness is a complex and individualized concept that can vary greatly from person to person.', response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 23, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,  # get the topic from first chain\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "} | RunnableLambda(topic_route)\n",
    "\n",
    "full_chain.invoke({\"question\": \"what is happiness?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINDING ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "/var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/ipykernel_79679/841432439.py:5: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "  \"Write out 4 flashcard questions with their options based on the topic given below.. Use the format\\n\\QUESTIONS:...\\nOPTIONS:...\\nSOLUTION:...\\n\\n\",\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out 4 flashcard questions with their options based on the topic given below.. Use the format\\n\\QUESTIONS:...\\nOPTIONS:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to invoke using a dict\n",
    "chain_direct = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to invoke is to use runnable\n",
    "chain_runnable = (\n",
    "    {\n",
    "        \"topic\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind(stop=\"SOLUTION\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTIONS:\n",
      "1. What year did the Cuban Missile Crisis take place?\n",
      "2. Who was the President of the United States during the Cuban Missile Crisis?\n",
      "3. Which country placed nuclear missiles in Cuba, triggering the crisis?\n",
      "4. How did the United States respond to the discovery of missiles in Cuba?\n",
      "\n",
      "OPTIONS:\n",
      "A. 1961\n",
      "B. 1962\n",
      "C. 1963\n",
      "D. John F. Kennedy\n",
      "E. Richard Nixon\n",
      "F. Lyndon B. Johnson\n",
      "G. Soviet Union\n",
      "H. China\n",
      "I. United Kingdom\n",
      "J. Diplomatic negotiations\n",
      "K. Military invasion\n",
      "L. Naval blockade\n",
      "\n",
      "SOLUTION:\n",
      "1. B. 1962\n",
      "2. D. John F. Kennedy\n",
      "3. G. Soviet Union\n",
      "4. L. Naval blockade\n"
     ]
    }
   ],
   "source": [
    "print(chain_direct.invoke({\"topic\": \"the cuban missile crisis\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Who was the leader of the Soviet Union during the Cuban Missile Crisis?\n",
      "OPTIONS: \n",
      "A) Nikita Khrushchev\n",
      "B) Joseph Stalin\n",
      "C) Vladimir Putin\n",
      "D) Mikhail Gorbachev\n",
      "QUESTION: Which US President was in office during the Cuban Missile Crisis?\n",
      "OPTIONS: \n",
      "A) John F. Kennedy\n",
      "B) Richard Nixon\n",
      "C) Dwight D. Eisenhower\n",
      "D) Lyndon B. Johnson\n",
      "QUESTION: Which country did the United States discover was installing nuclear missiles in Cuba, leading to the crisis?\n",
      "OPTIONS: \n",
      "A) Soviet Union\n",
      "B) China\n",
      "C) North Korea\n",
      "D) Cuba\n",
      "QUESTION: How did the Cuban Missile Crisis end?\n",
      "OPTIONS: \n",
      "A) The United States launched a military strike on Cuba\n",
      "B) The Soviet Union agreed to remove the missiles from Cuba in exchange for the US removing missiles from Turkey\n",
      "C) The United Nations intervened and brokered a peace deal\n",
      "D) Cuba declared war on the United States\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain_runnable.invoke(\"the cuban missile crisis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPENAI FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# how the function might look like in python\n",
    "import re\n",
    "\n",
    "def return_questions(raw_text):\n",
    "    \"\"\"\n",
    "    Extracts questions from raw text.\n",
    "\n",
    "    Parameters:\n",
    "    raw_text (str): The raw text of flashcard questions.\n",
    "\n",
    "    Returns:\n",
    "    list: An array of questions extracted from the raw text.\n",
    "    \"\"\"\n",
    "    # Regular expression to find sentences ending with a question mark\n",
    "    question_pattern = r'\\b(?:\\w+\\b[ \\t]*)+[?]'\n",
    "    # Find all occurrences of the pattern\n",
    "    questions = re.findall(question_pattern, raw_text)\n",
    "\n",
    "    return questions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {\n",
    "    \"name\": \"return_questions\",\n",
    "    \"description\": \"extracts questions from raw text\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"raw_text\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The raw text of flashcard questions\",\n",
    "            },\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"array of questions\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"question string\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Write out flashcard questions to the following topic then return the list of questions.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='{topic}'))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_function_calling = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out flashcard questions to the following topic then return the list of questions.\",\n",
    "        ),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_function_calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_function_calling = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0).bind(\n",
    "    function_call={\"name\": \"return_questions\"}, functions=[function]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_function_calling = (\n",
    "    {\"topic\": RunnablePassthrough()} | prompt_function_calling | llm_function_calling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"raw_text\": \"1. When did World War 2 start?\\\\n2. Which countries were the major powers in World War 2?\\\\n3. Who was the leader of Nazi Germany during World War 2?\\\\n4. What event led to the United States joining World War 2?\\\\n5. When did World War 2 end?\\\\n6. What was the Holocaust?\\\\n7. What were the major battles of World War 2?\\\\n8. How many people died in World War 2?\\\\n9. What were the consequences of World War 2?\\\\n10. Who were the Allied Powers and the Axis Powers in World War 2?\"\\n}', 'name': 'return_questions'}}, response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 95, 'total_tokens': 237}, 'model_name': 'gpt-3.5-turbo-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pp\n",
    "\n",
    "pp(chain_function_calling.invoke({\"topic\": \"world war 2\"}), width=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "request = httpx.Request(\"GET\", \"/\")\n",
    "response = httpx.Response(200, request=request)\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max to 0 to not retry\n",
    "llm_openai = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, max_retries=0)\n",
    "llm_anthropic = ChatAnthropic(\n",
    "    anthropic_api_key=ANTHROPIC_API_KEY, temperature=0)\n",
    "\n",
    "llm = llm_openai.with_fallbacks([llm_anthropic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit error\n"
     ]
    }
   ],
   "source": [
    "# with no fallback\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm_openai.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" I don't know, why did the chicken cross the road?\"\n"
     ]
    }
   ],
   "source": [
    "# Now let's try with fallbacks to Anthropic\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['animal'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You're a nice assistant who always includes a compliment in your response\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['animal'], template='Why did the {animal} cross the road?'))])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" I don't actually know why the kangaroo crossed the road. I'm an AI assistant without information about the motivations of specific kangaroos. But I'm happy to play along with some lighthearted humor! What's your guess for why the kangaroo crossed the road?\"\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"animal\": \"kangaroo\"}))\n",
    "    except RateLimitError:\n",
    "        print(\"Hit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "# Here we're going to use a bad model name to easily create a chain that will error\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = OpenAI()\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nResponse: That's a very clever question! I love your sense of humor and wordplay.\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"animal\": \"turtle\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
