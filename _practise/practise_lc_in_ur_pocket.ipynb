{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGCHAIN IN YOUR POCKET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe shortest war in history was between Great Britain and Zanzibar in 1896, lasting only 38 minutes.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "llm.invoke(\"Tell me a fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Models and Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    # instructions/context\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What should we do to stop pollution\"),  # question\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nSystem: There are many things we can do to stop pollution. Some ideas include reducing our use of single-use plastics, carpooling or using public transportation, supporting renewable energy sources, and properly disposing of waste. It's also important to educate others about the impact of pollution and encourage them to make changes in their daily lives.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['domain', 'number'], template='suggest {number} names for a {domain} startup?')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_general = \"suggest {number} names for a {domain} startup?\"\n",
    "\n",
    "prompt_general = PromptTemplate.from_template(template=template_general)\n",
    "prompt_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_lcel = prompt_general | llm\n",
    "chain_regular = LLMChain(llm=llm, prompt=prompt_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '5', 'domain': 'cooking', 'text': '\\n\\n1. \"Sizzle Kitchen Co.\"\\n2. \"Flavor Fusion Foods\"\\n3. \"Taste Haven Co.\"\\n4. \"Culinary Creations Co.\"\\n5. \"Savory Solutions Co.\"'}\n"
     ]
    }
   ],
   "source": [
    "result = chain_regular.invoke(\n",
    "    {\n",
    "        \"number\": \"5\",\n",
    "        \"domain\": \"cooking\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['category'], template='You are a helpful assistant who can give {category} for given input'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system template\n",
    "template_system = \"You are a helpful assistant who can give {category} for given input\"\n",
    "\n",
    "prompt_system = SystemMessagePromptTemplate.from_template(template_system)\n",
    "prompt_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# human template\n",
    "template_human = \"{text}\"\n",
    "prompt_human = HumanMessagePromptTemplate.from_template(template_human)\n",
    "prompt_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['category', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['category'], template='You are a helpful assistant who can give {category} for given input')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a chat prompt using system, human templates\n",
    "prompt_chat = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        prompt_system,\n",
    "        prompt_human,\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12f794bc0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12f41b800>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "llm_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Polite', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 15, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat.invoke(input=[\"antonyms\", \"Rude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Models and Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['domain', 'number'], template='Give {number} names for a {domain} startup?'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x12fe95ac0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x12fea37d0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Give {number} names for a {domain} startup?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': '2',\n",
       " 'domain': 'AI',\n",
       " 'text': '\\n\\n1. \"NeuroNexus\"\\n2. \"Cognitive Catalyst\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"number\": \"2\",\n",
    "        \"domain\": \"AI\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MathsChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "chain_math = LLMMathChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "what is the 3rd root of 1498?\u001b[32;1m\u001b[1;3m```text\n",
      "1498**(1/3)\n",
      "```\n",
      "...numexpr.evaluate(\"1498**(1/3)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m11.442052543837162\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the 3rd root of 1498?',\n",
       " 'answer': 'Answer: 11.442052543837162'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_math.invoke(\"what is the 3rd root of 1498?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['person', 'sentence'], template=\"\\nWhat is the city of this {person}? \\n\\nTranslate: {sentence} in the city's native language\\n\"))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create prompt\n",
    "template = \"\"\"\n",
    "What is the city of this {person}? \\n\n",
    "Translate: {sentence} in the city's native language\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12b419c10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12b41ae70>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Virat Kohli is Delhi.\\n\\nIn Hindi, \"what is going on bud?\" would be translated as \"भाई, क्या चल रहा है?\"'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# call chain\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Virat Kohli\",\n",
    "        \"sentence\": \"what is going on bud?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Virat Kohli is Delhi.\\n\\nTranslation: शहर में क्या हो रहा है दोस्त? (Shahar mein kya ho raha hai dost?)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create chain in another way\n",
    "from operator import itemgetter\n",
    "\n",
    "chain2 = (\n",
    "    {\"person\": itemgetter(\"person\"), \"sentence\": itemgetter(\"sentence\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke(\n",
    "    {\n",
    "        \"person\": \"Virat Kohli\",\n",
    "        \"sentence\": \"what is going on bud?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompts\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"Which country won the {game} world cup recently?\"\n",
    ")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Suggest the best {entity} from {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12b41bdd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12b4a80b0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chains\n",
    "from operator import itemgetter\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\n",
    "        \"country\": chain1,\n",
    "        \"entity\": itemgetter(\"entity\"),\n",
    "    }\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The best dish from France, the winner of the most recent FIFA World Cup in 2018, would be Coq au Vin. This classic French dish features chicken braised in red wine with mushrooms, onions, and bacon, resulting in a rich and flavorful stew that is sure to impress. Serve it with a side of crusty bread or mashed potatoes for a truly delicious meal.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke chains\n",
    "chain2.invoke({\"game\": \"football\", \"entity\": \"dish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.tools import YouTubeSearchTool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-fullstackretriever-greg/.venv/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube = YouTubeSearchTool()\n",
    "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"youtube\",\n",
    "        func=youtube.run,\n",
    "        description=\"Search for a YouTube video\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"wiki\",\n",
    "        func=wiki.run,\n",
    "        description=\"Search Wikipedia\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Answer the question asked. you have access to the following tools:\"\n",
    "\n",
    "suffix = \"Begin! \\\n",
    "{chat_history} \\\n",
    "Question: {input} \\\n",
    "{agent_scratchpad}\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-fullstackretriever-greg/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain.agents.mrkl.base.ZeroShotAgent` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use create_react_agent instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/pract-fullstackretriever-greg/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: I should use the tools available to find information on creating a RAG based llm bot using langchain.\n",
      "Action: wiki\n",
      "Action Input: RAG based llm bot using langchain\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Prompt engineering\n",
      "Summary: Prompt engineering is the process of structuring an instruction that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\n",
      "A prompt for a text-to-text language model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison → house, chat → cat, chien →\" (the expected response being dog), an approach called few-shot learning.\n",
      "When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should also search for a YouTube video to learn more about creating a RAG based llm bot using langchain.\n",
      "Action: youtube\n",
      "Action Input: RAG based llm bot using langchain\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['https://www.youtube.com/watch?v=LhnCsygAvzY&pp=ygUhUkFHIGJhc2VkIGxsbSBib3QgdXNpbmcgbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=O60-KuZZeQA&pp=ygUhUkFHIGJhc2VkIGxsbSBib3QgdXNpbmcgbGFuZ2NoYWlu']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: To create a RAG based llm bot using langchain, you need to use prompt engineering to structure an instruction for the AI model and provide relevant examples for few-shot learning. You can also watch YouTube videos for more information and guidance on this topic.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To create a RAG based llm bot using langchain, you need to use prompt engineering to structure an instruction for the AI model and provide relevant examples for few-shot learning. You can also watch YouTube videos for more information and guidance on this topic.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\n",
    "    input=\"Tell me how create a RAG based llm bot using langchain, also get me a link on youtube on this\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
