{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGCHAIN IN YOUR POCKET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe shortest war in history was between Great Britain and Zanzibar in 1896, lasting only 38 minutes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "llm.invoke(\"Tell me a fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Models and Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatModel\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    # instructions/context\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What should we do to stop pollution\"),  # question\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nSystem: There are many things we can do to stop pollution. Some ideas include reducing our use of single-use plastics, carpooling or using public transportation, supporting renewable energy sources, and properly disposing of waste. It's also important to educate others about the impact of pollution and encourage them to make changes in their daily lives.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['domain', 'number'], template='suggest {number} names for a {domain} startup?')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_general = \"suggest {number} names for a {domain} startup?\"\n",
    "\n",
    "prompt_general = PromptTemplate.from_template(template=template_general)\n",
    "prompt_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_lcel = prompt_general | llm\n",
    "chain_regular = LLMChain(llm=llm, prompt=prompt_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '5', 'domain': 'cooking', 'text': '\\n\\n1. \"Sizzle Kitchen Co.\"\\n2. \"Flavor Fusion Foods\"\\n3. \"Taste Haven Co.\"\\n4. \"Culinary Creations Co.\"\\n5. \"Savory Solutions Co.\"'}\n"
     ]
    }
   ],
   "source": [
    "result = chain_regular.invoke(\n",
    "    {\n",
    "        \"number\": \"5\",\n",
    "        \"domain\": \"cooking\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['category'], template='You are a helpful assistant who can give {category} for given input'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system template\n",
    "template_system = \"You are a helpful assistant who can give {category} for given input\"\n",
    "\n",
    "prompt_system = SystemMessagePromptTemplate.from_template(template_system)\n",
    "prompt_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# human template\n",
    "template_human = \"{text}\"\n",
    "prompt_human = HumanMessagePromptTemplate.from_template(template_human)\n",
    "prompt_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['category', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['category'], template='You are a helpful assistant who can give {category} for given input')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a chat prompt using system, human templates\n",
    "prompt_chat = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        prompt_system,\n",
    "        prompt_human,\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12f794bc0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12f41b800>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "llm_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Polite', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 15, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat.invoke(input=[\"antonyms\", \"Rude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Models and Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['domain', 'number'], template='Give {number} names for a {domain} startup?'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x12fe95ac0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x12fea37d0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Give {number} names for a {domain} startup?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': '2',\n",
       " 'domain': 'AI',\n",
       " 'text': '\\n\\n1. \"NeuroNexus\"\\n2. \"Cognitive Catalyst\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"number\": \"2\",\n",
    "        \"domain\": \"AI\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MathsChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "chain_math = LLMMathChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "what is the 3rd root of 1498?\u001b[32;1m\u001b[1;3m```text\n",
      "1498**(1/3)\n",
      "```\n",
      "...numexpr.evaluate(\"1498**(1/3)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m11.442052543837162\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the 3rd root of 1498?',\n",
       " 'answer': 'Answer: 11.442052543837162'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_math.invoke(\"what is the 3rd root of 1498?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['person', 'sentence'], template=\"\\nWhat is the city of this {person}? \\n\\nTranslate: {sentence} in the city's native language\\n\"))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create prompt\n",
    "template = \"\"\"\n",
    "What is the city of this {person}? \\n\n",
    "Translate: {sentence} in the city's native language\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12b419c10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12b41ae70>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Virat Kohli is Delhi.\\n\\nIn Hindi, \"what is going on bud?\" would be translated as \"भाई, क्या चल रहा है?\"'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# call chain\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Virat Kohli\",\n",
    "        \"sentence\": \"what is going on bud?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Virat Kohli is Delhi.\\n\\nTranslation: शहर में क्या हो रहा है दोस्त? (Shahar mein kya ho raha hai dost?)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create chain in another way\n",
    "from operator import itemgetter\n",
    "\n",
    "chain2 = (\n",
    "    {\"person\": itemgetter(\"person\"), \"sentence\": itemgetter(\"sentence\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke(\n",
    "    {\n",
    "        \"person\": \"Virat Kohli\",\n",
    "        \"sentence\": \"what is going on bud?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompts\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"Which country won the {game} world cup recently?\"\n",
    ")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Suggest the best {entity} from {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12b41bdd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12b4a80b0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chains\n",
    "from operator import itemgetter\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\n",
    "        \"country\": chain1,\n",
    "        \"entity\": itemgetter(\"entity\"),\n",
    "    }\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The best dish from France, the winner of the most recent FIFA World Cup in 2018, would be Coq au Vin. This classic French dish features chicken braised in red wine with mushrooms, onions, and bacon, resulting in a rich and flavorful stew that is sure to impress. Serve it with a side of crusty bread or mashed potatoes for a truly delicious meal.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke chains\n",
    "chain2.invoke({\"game\": \"football\", \"entity\": \"dish\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
